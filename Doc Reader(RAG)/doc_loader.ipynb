{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23726fd1",
   "metadata": {},
   "source": [
    "## Question Answers on private document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f708e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "test = load_dotenv(dotenv_path=\"../ML_practice/cred.env\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd63f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ea2754e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb3b84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install docx2txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ebf8e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wikipedia -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a824fdb",
   "metadata": {},
   "source": [
    "#### Load pdf python function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b6de57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    from langchain_classic.document_loaders import PyPDFLoader\n",
    "    print(f'Loading of {file}')\n",
    "    loader = PyPDFLoader(file)\n",
    "    data = loader.load()\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eac7d55c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading of us_constitution.pdf\n"
     ]
    }
   ],
   "source": [
    "data = load_document('us_constitution.pdf')\n",
    "\n",
    "#data = load_document('us_constitution.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c08bc607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No of pages:41\n"
     ]
    }
   ],
   "source": [
    "print(f'No of pages:{len(data)}')\n",
    "#print(f'No of pages:{data[23]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f0556c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample page:The United States shall guarantee to every State in this Union a \n",
      " Republican Form of Government, and shall protect each of them \n",
      " against Invasion; and on Application of the Legislature, or of the \n",
      " Executive (when the Legislature cannot be convened) against domestic \n",
      " V iolence. \n",
      "Article V \n",
      " The Congress, whenever two thirds of both Houses shall deem it \n",
      " necessary , shall propose Amendments to this Constitution, or , on the \n",
      " Application of the Legislatures of two thirds of the several States, shall \n",
      " call a Convention for proposing Amendments, which, in either Case, \n",
      " shall be valid to all Intents and Purposes, as Part of this Constitution, \n",
      " when ratified by the Legislatures of three fourths of the several States, \n",
      " or by Conventions in three fourths thereof, as the one or the other \n",
      " Mode of Ratification may be proposed by the Congress; Provided that \n",
      " no Amendment which may be made prior to the Y ear One thousand \n",
      " eight hundred and eight shall in any Manner af fect the first and fourth \n",
      " Clauses in the Ninth Section of the first Article; and that no State, \n",
      " without its Consent, shall be deprived of its equal Suf frage in the \n",
      " Senate. \n",
      "Article VI\n"
     ]
    }
   ],
   "source": [
    "print(f'sample page:{data[20].page_content}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8432e5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(file):\n",
    "    import os\n",
    "    name, extension = os.path.splitext(file)\n",
    "\n",
    "    if extension == '.pdf':\n",
    "        from langchain_classic.document_loaders import PyPDFLoader\n",
    "        print(f'Loading of {file}')\n",
    "        loader = PyPDFLoader(file)\n",
    "    elif extension == '.docx':\n",
    "        from langchain_classic.document_loaders import Docx2txtLoader\n",
    "        print(f'Loading of {file} with extension {extension}')\n",
    "        loader = Docx2txtLoader(file)\n",
    "    else:\n",
    "        print(f'Document format {extension} is NOt supported')\n",
    "        return None\n",
    "    \n",
    "    data = loader.load()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85952c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid pdf header: b'PK\\x03\\x04\\x14'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading of LLM Engineers Handbook_split2.docx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EOF marker not found\n"
     ]
    },
    {
     "ename": "PdfStreamError",
     "evalue": "Stream has ended unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPdfStreamError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data2 \u001b[38;5;241m=\u001b[39m load_document(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLM Engineers Handbook_split2.docx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 5\u001b[0m, in \u001b[0;36mload_document\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m loader \u001b[38;5;241m=\u001b[39m PyPDFLoader(file)\n\u001b[1;32m----> 5\u001b[0m data \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\prach\\anaconda3\\Lib\\site-packages\\langchain_core\\document_loaders\\base.py:43\u001b[0m, in \u001b[0;36mBaseLoader.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m     38\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load data into `Document` objects.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    Returns:\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124;03m        The documents.\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy_load())\n",
      "File \u001b[1;32mc:\\Users\\prach\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\pdf.py:305\u001b[0m, in \u001b[0;36mPyPDFLoader.lazy_load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    304\u001b[0m     blob \u001b[38;5;241m=\u001b[39m Blob\u001b[38;5;241m.\u001b[39mfrom_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n\u001b[1;32m--> 305\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser\u001b[38;5;241m.\u001b[39mlazy_parse(blob)\n",
      "File \u001b[1;32mc:\\Users\\prach\\anaconda3\\Lib\\site-packages\\langchain_community\\document_loaders\\parsers\\pdf.py:384\u001b[0m, in \u001b[0;36mPyPDFParser.lazy_parse\u001b[1;34m(self, blob)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m page\u001b[38;5;241m.\u001b[39mextract_text(\n\u001b[0;32m    379\u001b[0m             extraction_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextraction_mode,\n\u001b[0;32m    380\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextraction_kwargs,\n\u001b[0;32m    381\u001b[0m         )\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m blob\u001b[38;5;241m.\u001b[39mas_bytes_io() \u001b[38;5;28;01mas\u001b[39;00m pdf_file_obj:\n\u001b[1;32m--> 384\u001b[0m     pdf_reader \u001b[38;5;241m=\u001b[39m pypdf\u001b[38;5;241m.\u001b[39mPdfReader(pdf_file_obj, password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpassword)\n\u001b[0;32m    386\u001b[0m     doc_metadata \u001b[38;5;241m=\u001b[39m _purge_metadata(\n\u001b[0;32m    387\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproducer\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyPDF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyPDF\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreationdate\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;241m|\u001b[39m cast(\u001b[38;5;28mdict\u001b[39m, pdf_reader\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mor\u001b[39;00m {})\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m         }\n\u001b[0;32m    393\u001b[0m     )\n\u001b[0;32m    394\u001b[0m     single_texts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\prach\\anaconda3\\Lib\\site-packages\\pypdf\\_reader.py:131\u001b[0m, in \u001b[0;36mPdfReader.__init__\u001b[1;34m(self, stream, strict, password)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_id2num: Optional[\u001b[38;5;28mdict\u001b[39m[Any, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validated_root: Optional[DictionaryObject] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_stream(stream)\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_objects: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_override_encryption \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prach\\anaconda3\\Lib\\site-packages\\pypdf\\_reader.py:153\u001b[0m, in \u001b[0;36mPdfReader._initialize_stream\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    151\u001b[0m         stream \u001b[38;5;241m=\u001b[39m BytesIO(fh\u001b[38;5;241m.\u001b[39mread())\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_opened \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(stream)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m stream\n",
      "File \u001b[1;32mc:\\Users\\prach\\anaconda3\\Lib\\site-packages\\pypdf\\_reader.py:592\u001b[0m, in \u001b[0;36mPdfReader.read\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;124;03mRead and process the PDF stream, extracting necessary data.\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    589\u001b[0m \n\u001b[0;32m    590\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_basic_validation(stream)\n\u001b[1;32m--> 592\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_eof_marker(stream)\n\u001b[0;32m    593\u001b[0m startxref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_startxref_pos(stream)\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_startxref \u001b[38;5;241m=\u001b[39m startxref\n",
      "File \u001b[1;32mc:\\Users\\prach\\anaconda3\\Lib\\site-packages\\pypdf\\_reader.py:698\u001b[0m, in \u001b[0;36mPdfReader._find_eof_marker\u001b[1;34m(self, stream)\u001b[0m\n\u001b[0;32m    696\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m PdfReadError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEOF marker not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    697\u001b[0m     logger_warning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEOF marker not found\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m--> 698\u001b[0m line \u001b[38;5;241m=\u001b[39m read_previous_line(stream)\n",
      "File \u001b[1;32mc:\\Users\\prach\\anaconda3\\Lib\\site-packages\\pypdf\\_utils.py:316\u001b[0m, in \u001b[0;36mread_previous_line\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    314\u001b[0m found_crlf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PdfStreamError(STREAM_TRUNCATED_PREMATURELY)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    318\u001b[0m     to_read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(DEFAULT_BUFFER_SIZE, stream\u001b[38;5;241m.\u001b[39mtell())\n",
      "\u001b[1;31mPdfStreamError\u001b[0m: Stream has ended unexpectedly"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data2 = load_document('LLM Engineers Handbook_split2.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fc4070d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no of pages :1\n"
     ]
    }
   ],
   "source": [
    "print(f'no of pages :{len(data2)}') ##adding all pages into one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102fb33e",
   "metadata": {},
   "source": [
    "## load_from_wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2c20c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## wikipedia loader\n",
    "\n",
    "def load_from_wikipedia(query, lang='en',load_max_docs=2):\n",
    "    from langchain_classic.document_loaders import WikipediaLoader\n",
    "\n",
    "    loader = WikipediaLoader(query=query,lang=lang,load_max_docs=load_max_docs)\n",
    "\n",
    "    data = loader.load()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cdc6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = load_from_wikipedia('Large language model','es')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11259e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75bbb4",
   "metadata": {},
   "source": [
    "## chunk_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fda2c872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_data(data,chunk_size=750,chunk_overlap=100):\n",
    "    from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "    print(f'Chunking data with chunk_size:{chunk_size} and chunk_overlap:{chunk_overlap}')\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    chunks = text_splitter.split_documents(data)\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91ebc283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1321138c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Results ==\n",
      "Numbers in parentheses in the table are Super Bowl appearances as of the date of that Super Bowl and are used as follows:\n",
      "\n",
      "Winning team and Losing team columns indicate the number of times that team has appeared in a Super Bowl as well as each respective teams' Super Bowl record to date.\n",
      "Venue column indicates number of times that stadium has hosted a Super Bowl.\n",
      "City column indicates number of times that metropolitan area has hosted a Super Bowl.\n",
      "\n",
      "W  Indicates a team that made the playoffs as a wild card team (rather than by winning a division).\n",
      "\n",
      "\n",
      "=== Upcoming games ===\n",
      "\n",
      "\n",
      "==== Notes ====\n",
      "\n",
      "\n",
      "== Streaks and rematches ==\n",
      "\n",
      "\n",
      "=== Consecutive wins ===\n"
     ]
    }
   ],
   "source": [
    "print(chunks[11].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2e983bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_cost(texts):\n",
    "    import tiktoken\n",
    "    enc= tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "\n",
    "    total_tokens =  sum([len(enc.encode(page.page_content)) for page in texts])\n",
    "\n",
    "    print(f'total_tokens:{total_tokens}')\n",
    "\n",
    "    print(f'Embedding cost USD: {total_tokens/1000 * 0.00002 :.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5ca3f959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_tokens:16567\n",
      "Embedding cost USD: 0.000331\n"
     ]
    }
   ],
   "source": [
    "get_embedding_cost(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb27f66",
   "metadata": {},
   "source": [
    "## Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa3a00af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../ML_practice/cred.env\")\n",
    "\n",
    "PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06c36a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pinecone-client -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7696d754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pinecone-client\n",
      "Version: 6.0.0\n",
      "Summary: Pinecone client (DEPRECATED)\n",
      "Home-page: https://www.pinecone.io\n",
      "Author: Pinecone Systems, Inc.\n",
      "Author-email: support@pinecone.io\n",
      "License: Apache-2.0\n",
      "Location: c:\\Users\\prach\\anaconda3\\Lib\\site-packages\n",
      "Requires: certifi, pinecone-plugin-interface, python-dateutil, typing-extensions, urllib3\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pinecone-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d978b8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pinecone -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a076f2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pinecone\n",
      "Version: 7.3.0\n",
      "Summary: Pinecone client and SDK\n",
      "Home-page: https://www.pinecone.io\n",
      "Author: Pinecone Systems, Inc.\n",
      "Author-email: support@pinecone.io\n",
      "License: Apache-2.0\n",
      "Location: c:\\Users\\prach\\anaconda3\\Lib\\site-packages\n",
      "Requires: certifi, pinecone-plugin-assistant, pinecone-plugin-interface, python-dateutil, typing-extensions, urllib3\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0adcd0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "    {\n",
       "        \"name\": \"askdoc\",\n",
       "        \"metric\": \"cosine\",\n",
       "        \"host\": \"askdoc-huqjqmm.svc.aped-4627-b74a.pinecone.io\",\n",
       "        \"spec\": {\n",
       "            \"serverless\": {\n",
       "                \"cloud\": \"aws\",\n",
       "                \"region\": \"us-east-1\"\n",
       "            }\n",
       "        },\n",
       "        \"status\": {\n",
       "            \"ready\": true,\n",
       "            \"state\": \"Ready\"\n",
       "        },\n",
       "        \"vector_type\": \"dense\",\n",
       "        \"dimension\": 1536,\n",
       "        \"deletion_protection\": \"disabled\",\n",
       "        \"tags\": null\n",
       "    }\n",
       "]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "pc.list_indexes()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ccf13cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['langchain']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pc.list_indexes().names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ecd0d12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create New index langchain\n",
      "Index Created\n"
     ]
    }
   ],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "index_name = 'langchain'\n",
    "\n",
    "if index_name not in pc.list_indexes().names() :\n",
    "    print (f'create New index {index_name}')\n",
    "    pc.create_index(\n",
    "        name = index_name,\n",
    "        dimension =1536,\n",
    "        metric = 'cosine',\n",
    "        spec =ServerlessSpec(\n",
    "            cloud='aws',\n",
    "            region='us-east-1'\n",
    "        )\n",
    "\n",
    "    )\n",
    "    print('Index Created')\n",
    "else:\n",
    "    print (f'index {index_name} already exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "18b877c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete index langchain\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "index_name = 'langchain'\n",
    "\n",
    "if index_name in pc.list_indexes().names() :\n",
    "    print (f'Delete index {index_name}')\n",
    "    pc.delete_index(index_name)\n",
    "    print('Done')\n",
    "else:\n",
    "    print (f'index {index_name} does NOT exists')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f84d151",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'': {'vector_count': 375}},\n",
       " 'total_vector_count': 375,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pc.Index(index_name)\n",
    "\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c66616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 5}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## insert vector\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "vectors = [[random.random() for _ in range(1536)] for v in range(5)]\n",
    "\n",
    "ids = list('abcde')\n",
    "\n",
    "index_name = 'langchain'\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "index.upsert(vectors=zip(ids,vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa2402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 1}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## update vector\n",
    "\n",
    "index.upsert(vectors=[('c',[0.5]*1536)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b84fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fetch vector\n",
    "\n",
    "index.fetch(ids=['d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f747b3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index delete\n",
    "index.delete(ids=['d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "809d185d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'': {'vector_count': 4}},\n",
       " 'total_vector_count': 4,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "be0c8771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FetchResponse(namespace='', vectors={}, usage={'read_units': 1})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.fetch(ids=['t'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "764f4b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## query \n",
    "\n",
    "query_vector = [random.random() for _ in range(1536)]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7dfb255",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.query(query_vector, top_k=2, include_values=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "77f90494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'c', 'score': 0.866620064, 'values': []},\n",
       "             {'id': 'e', 'score': 0.749157429, 'values': []}],\n",
       " 'namespace': '',\n",
       " 'usage': {'read_units': 1}}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.query(vector=query_vector, top_k=2, include_values=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "59c25132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 2}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = [[random.random() for _ in range(1536)] for v in range(2)]\n",
    "\n",
    "ids = list('xy')\n",
    "\n",
    "index_name = 'langchain'\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "index.upsert(vectors=zip(ids,vectors),namespace='fstnm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9ce60e6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 3}"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = [[random.random() for _ in range(1536)] for v in range(3)]\n",
    "\n",
    "ids = list('123')\n",
    "\n",
    "index_name = 'langchain'\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "index.upsert(vectors=zip(ids,vectors),namespace='secndnm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "8790b33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'': {'vector_count': 4},\n",
       "                'fstnm': {'vector_count': 2},\n",
       "                'secndnm': {'vector_count': 3}},\n",
       " 'total_vector_count': 9,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395e4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.fetch(ids=['x'],namespace='fstnm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b6c7f4f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.delete(ids=['x'],namespace='fstnm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "015ed7e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.delete(delete_all=True,namespace='fstnm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "7c0c2e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'metric': 'cosine',\n",
       " 'namespaces': {'': {'vector_count': 4}, 'secndnm': {'vector_count': 3}},\n",
       " 'total_vector_count': 7,\n",
       " 'vector_type': 'dense'}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d14f6124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 0.3.27 requires langchain-core<1.0.0,>=0.3.72, but you have langchain-core 1.0.3 which is incompatible.\n",
      "langchain-classic 1.0.0 requires langchain-text-splitters<2.0.0,>=1.0.0, but you have langchain-text-splitters 0.3.11 which is incompatible.\n",
      "langchain-experimental 0.3.4 requires langchain-core<0.4.0,>=0.3.28, but you have langchain-core 1.0.3 which is incompatible.\n",
      "langchain-google-genai 3.0.0 requires google-ai-generativelanguage<1.0.0,>=0.7.0, but you have google-ai-generativelanguage 0.6.15 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain-pinecone -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d234d6",
   "metadata": {},
   "source": [
    "## insert_or_fetch_embeddings into vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92dcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade langchain-pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e4b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_or_fetch_embeddings(index_name,chunks):\n",
    "    import pinecone\n",
    "    from pinecone import Pinecone\n",
    "    from langchain_pinecone import PineconeVectorStore\n",
    "    #from langchain_openai import OpenAiEmbeddings\n",
    "    from pinecone import PodSpec,ServerlessSpec\n",
    "    from langchain_classic.embeddings.openai import OpenAIEmbeddings\n",
    "    import os\n",
    "\n",
    "    PINECONE_API_KEY = os.getenv('PINECONE_API_KEY')\n",
    "\n",
    "    pc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small',dimensions=1536)\n",
    "\n",
    "\n",
    "    if index_name in pc.list_indexes().names() :\n",
    "        print (f'index {index_name} already exists, ..fetch embeddings')\n",
    "        vector_store = PineconeVectorStore.from_existing_index(\n",
    "            index_name=index_name,\n",
    "            embedding=embeddings\n",
    "            #pinecone_api_key=PINECONE_API_KEY,\n",
    "        )\n",
    "        print(\"Done\")\n",
    "    else:\n",
    "        print (f'index {index_name} does NOT exists, create new index')\n",
    "\n",
    "        pc.create_index(\n",
    "            name = index_name,\n",
    "            dimension=1536,\n",
    "            metric = 'cosine',\n",
    "            spec =ServerlessSpec(\n",
    "                cloud='aws',\n",
    "                region='us-east-1'\n",
    "            )\n",
    "\n",
    "        )\n",
    "        print('Index Created')\n",
    "\n",
    "        vector_store = PineconeVectorStore.from_documents(\n",
    "            documents=chunks,\n",
    "            embedding=embeddings,\n",
    "            index_name=index_name\n",
    "            #pinecone_api_key=PINECONE_API_KEY,\n",
    "        )\n",
    "        print(\"Done\")\n",
    "        \n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94377173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prach\\AppData\\Local\\Temp\\ipykernel_10544\\339275095.py:14: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the `langchain-openai package and should be used instead. To use it run `pip install -U `langchain-openai` and import as `from `langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model='text-embedding-3-small',dimensions=1536)\n",
      "c:\\Users\\prach\\anaconda3\\Lib\\site-packages\\langchain_community\\embeddings\\openai.py:271: UserWarning: WARNING! dimensions is not default parameter.\n",
      "                    dimensions was transferred to model_kwargs.\n",
      "                    Please confirm that dimensions is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index askdoc already exists, ..fetch embeddings\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "index_name = 'askdoc'\n",
    "\n",
    "vector_store = insert_or_fetch_embeddings(index_name,chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c93395d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## delete index\n",
    "\n",
    "def delete_pinecone_index(index_name ='all'):\n",
    "    import pinecone\n",
    "    pc = pinecone.Pinecone()\n",
    "    if index_name == 'all':\n",
    "        indexes = pc.list_indexes().names()\n",
    "        print(f'delete all indexes')\n",
    "        for ind in indexes:\n",
    "            pc.delete_index(ind)\n",
    "        print('Done')\n",
    "    else:\n",
    "        print(f'Deleting index {index_name}')\n",
    "        pc.delete_index(index_name)\n",
    "        print(f'Done')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "3c53e398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    }
   ],
   "source": [
    "chunks = chunk_data(data)\n",
    "\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f4a8ba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete all indexes\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "delete_pinecone_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "55b1676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade langchain langchain-core langchain-openai langchain-pinecone langchain-community langchain-classic langchain-experimental -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a86b358",
   "metadata": {},
   "source": [
    "### ask_to_rag  function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a80a369",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_to_rag(vector_store,question):\n",
    "    from langchain_classic.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-4o-mini',temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity',search_kwargs={'k':3})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type='stuff',retriever=retriever)\n",
    "\n",
    "    answer = chain.invoke(question)\n",
    "\n",
    "    return answer['result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "636afd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is about the United States Constitution. It outlines the purpose of the Constitution, which includes forming a better union, establishing justice, ensuring domestic tranquility, providing for the common defense, promoting general welfare, and securing rights under the law. It also mentions the authority of Congress to establish laws regarding the recognition of acts, records, and judicial proceedings from each state.\n"
     ]
    }
   ],
   "source": [
    "answer = ask_to_rag(vector_store,'what is this document about?')\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7a17c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example assumes vector_store implements similarity_search_with_score or similar\n",
    "def debug_retrieval(vector_store, question, k=5):\n",
    "    # Try different retriever methods depending on your vector_store API\n",
    "    # LangChain VectorStore interface often provides similarity_search_with_score\n",
    "    try:\n",
    "        results = vector_store.similarity_search_with_score(question, k=k)\n",
    "    except Exception:\n",
    "        # Fallback: use as_retriever + get docs from retriever\n",
    "        retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k':k})\n",
    "        docs = retriever.get_relevant_documents(question)\n",
    "        # No scores available here\n",
    "        results = [(doc, None) for doc in docs]\n",
    "\n",
    "    print(f\"=== Top {k} retrieval results for: {question} ===\")\n",
    "    for i, (doc, score) in enumerate(results, 1):\n",
    "        print(f\"\\n--- Result #{i} (score={score}) ---\")\n",
    "        print(\"metadata:\", getattr(doc, \"metadata\", None))\n",
    "        print(\"text snippet:\", (doc.page_content[:1000] + \"...\") if len(doc.page_content) > 1000 else doc.page_content)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(debug_retrieval(vector_store,'what is this document about?',k=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "799a3cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = input(\"Question {i}: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "488c1695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ask your question, Quit or exit to Quit\n",
      "\n",
      " Question 1: what does RAG means?\n",
      "\n",
      " Answer 1:  I don't know.\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      " Question 2: explain federalism\n",
      "\n",
      " Answer 2:  Federalism is a system of government in which power is divided between a central authority and regional or state governments. In a federal system, both the national and state governments have their own distinct responsibilities, powers, and functions. This division is designed to allow for a balance of power, preventing any single entity from becoming too powerful. \n",
      "\n",
      "In the United States, for example, the Constitution outlines the powers of the federal government and reserves certain powers for the states. This includes aspects such as law enforcement, education, transportation, and more, which can vary from state to state. The goal of federalism is to create a union that respects the autonomy of individual states while maintaining a cohesive national policy on issues that affect the entire country.\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      " Question 3: what is bill of rights\n",
      "\n",
      " Answer 3:  The Bill of Rights refers to the first ten amendments to the United States Constitution, which were ratified on December 15, 1791. These amendments guarantee essential rights and liberties to individuals and limit the power of the federal government. Key rights protected by the Bill of Rights include freedoms of speech, religion, and the press, the right to bear arms, protections against unreasonable searches and seizures, and rights related to criminal prosecutions, among others.\n",
      "\n",
      " --------------------------------------------------\n",
      "\n",
      " Goodbye\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "i=1\n",
    "\n",
    "print(\"Ask your question, Quit or exit to Quit\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    question = input(\"Question : \")\n",
    "    i = i+1\n",
    "    if question.lower() in ['quit', 'exit']:\n",
    "        print(\"\\n Goodbye\")\n",
    "        time.sleep(2)\n",
    "        break\n",
    "    else:\n",
    "        answer = ask_to_rag(vector_store,question)\n",
    "        print (f\"\\n Question {i-1}: {question}\")\n",
    "        print(f\"\\n Answer {i-1}: \",answer)\n",
    "        print(f\"\\n {'-'*50}\")\n",
    "        time.sleep(2)\n",
    "        print\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6cdb613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prach\\anaconda3\\Lib\\site-packages\\langchain_community\\embeddings\\openai.py:271: UserWarning: WARNING! dimensions is not default parameter.\n",
      "                    dimensions was transferred to model_kwargs.\n",
      "                    Please confirm that dimensions is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index askwiki does NOT exists, create new index\n",
      "Index Created\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "## wikipedia load\n",
    "\n",
    "\n",
    "data = load_from_wikipedia('superbowl', 'en')\n",
    "\n",
    "chunks = chunk_data(data)\n",
    "\n",
    "index_name = 'askwiki'\n",
    "vector_store = insert_or_fetch_embeddings(index_name,chunks)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "69dd7bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer: Halftime refers to the intermission period during a sporting event, particularly in football, where teams take a break between the two halves of the game. In the context of the Super Bowl, halftime is also known for featuring musical performances from top artists, which are a significant aspect of the event.\n"
     ]
    }
   ],
   "source": [
    "question = 'What is halftime?'\n",
    "answer = ask_to_rag(vector_store,question)\n",
    "\n",
    "print (f\"\\n Answer: {answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd337afe",
   "metadata": {},
   "source": [
    "# Implementation using Chroma db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b535cbe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q chromadb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9b7429",
   "metadata": {},
   "source": [
    "### Chroma db insert and retrive embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a901041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_embeddings_chromadb(chunks, persist_directory='./chroma_db'):\n",
    "    from langchain_classic.vectorstores import Chroma\n",
    "    from langchain_classic.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small',dimensions=1536)\n",
    "\n",
    "    print(\"Inserting embeddings\")\n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory)\n",
    "    print(\"Done\")\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5794f3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings_chromadb(persist_directory='./chroma_db'):\n",
    "    from langchain_classic.vectorstores import Chroma\n",
    "    from langchain_classic.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small',dimensions=1536)\n",
    "\n",
    "    vector_store = Chroma(embedding_function=embeddings, persist_directory=persist_directory)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f00b2",
   "metadata": {},
   "source": [
    "### load data in chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c98fbf11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading of LLM Engineers Handbook.pdf\n",
      "Chunking data with chunk_size:750 and chunk_overlap:100\n"
     ]
    }
   ],
   "source": [
    "data = load_document('LLM Engineers Handbook.pdf')\n",
    "\n",
    "chunks = chunk_data(data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e45cd922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prach\\anaconda3\\Lib\\site-packages\\langchain_community\\embeddings\\openai.py:271: UserWarning: WARNING! dimensions is not default parameter.\n",
      "                    dimensions was transferred to model_kwargs.\n",
      "                    Please confirm that dimensions is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting embeddings\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "vector_store = insert_embeddings_chromadb(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "76a7f9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer: The feature, training, and inference pipelines are distinct components of the FTI architecture, each serving a specific purpose:\n",
      "\n",
      "1. **Feature Pipeline**: This component is responsible for generating real-time features that are used for retrieval-augmented generation (RAG). It queries data from an online feature store, specifically from a vector database like Qdrant. The primary goal of the feature pipeline is to prepare and retrieve relevant features that will be used in the inference stage.\n",
      "\n",
      "2. **Training Pipeline**: This pipeline focuses on training machine learning models. It outputs trained model weights that are stored in a model registry. The training pipeline is resource-intensive, requiring powerful machines equipped with multiple GPUs to handle the computational demands of batching data and optimizing model weights. The performance of this pipeline is critical for producing accurate and effective ML models.\n",
      "\n",
      "3. **Inference Pipeline**: The inference pipeline serves the purpose of making predictions based on user input. It utilizes the real-time features provided by the feature pipeline and the trained models from the training pipeline to generate and serve predictions directly to users. The inference process is designed to be efficient and responsive to user queries.\n",
      "\n",
      "In summary, the feature pipeline generates features, the training pipeline builds and optimizes the models, and the inference pipeline applies these models to make predictions based on real-time data.\n"
     ]
    }
   ],
   "source": [
    "question = 'Explain difference bewteen feature , training and inference pipeline'\n",
    "answer = ask_to_rag(vector_store,question)\n",
    "\n",
    "print (f\"\\n Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "74192ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer: I don't know.\n"
     ]
    }
   ],
   "source": [
    "question = 'what is vertex AI search?'\n",
    "answer = ask_to_rag(vector_store,question)\n",
    "\n",
    "print (f\"\\n Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5db45771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer: AWS SageMaker is a fully managed machine learning service by AWS that enables developers and data scientists to build, train, and deploy machine learning models at scale. It simplifies the process by handling the underlying infrastructure, allowing users to focus on developing high-quality models efficiently. SageMaker is used to facilitate the training and deployment of ML models, making it easier to implement machine learning applications without needing to manage the hardware and software complexities involved.\n"
     ]
    }
   ],
   "source": [
    "question = 'what is AWS sagemaker, and why its used?'\n",
    "answer = ask_to_rag(vector_store,question)\n",
    "\n",
    "print (f\"\\n Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "23aa2089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer: The text does not provide specific strategies for avoiding hallucinations in RAG, so I don't know the answer to that question.\n"
     ]
    }
   ],
   "source": [
    "question = 'How to avoid hallucinations in RAG?'\n",
    "answer = ask_to_rag(vector_store,question)\n",
    "\n",
    "print (f\"\\n Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0d05506a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Answer: MLOps and LLMOps are both focused on managing machine learning models, but they address different aspects based on the type and complexity of the models being used.\n",
      "\n",
      "1. **Scope**: MLOps deals with machine learning models in general, while LLMOps is specifically focused on large language models (LLMs).\n",
      "\n",
      "2. **Complexity**: LLMOps addresses the unique challenges associated with LLMs, such as their large size, complex training requirements, and non-deterministic nature of generating answers. \n",
      "\n",
      "3. **Prompt Management**: LLMOps places particular emphasis on aspects like prompt monitoring and versioning, which are specific to LLM applications.\n",
      "\n",
      "4. **Retraining Decisions**: LLMOps requires critical decisions regarding changes in data distribution and when to retrain models, which are amplified due to the larger scale of LLMs.\n",
      "\n",
      "Overall, LLMOps builds on the foundation laid by MLOps but focuses on the distinct needs and challenges presented by large language models.\n"
     ]
    }
   ],
   "source": [
    "question = 'Difference bewtween MLops and LLMops'\n",
    "answer = ask_to_rag(vector_store,question)\n",
    "\n",
    "print (f\"\\n Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c6aa10",
   "metadata": {},
   "source": [
    "# Adding memeory to chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b813ce76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-preview',temperature=0)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type='similarity',search_kwargs={'k':3})\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    chain_type='stuff',\n",
    "    memory=memory,\n",
    "    retriever=retriever,\n",
    "    verbose=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42045b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question_conversation(question,chain):\n",
    "    result = chain.invoke({'question': question})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a490e851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading of LLM Engineers Handbook.pdf\n",
      "Chunking data with chunk_size:750 and chunk_overlap:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prach\\anaconda3\\Lib\\site-packages\\langchain_community\\embeddings\\openai.py:271: UserWarning: WARNING! dimensions is not default parameter.\n",
      "                    dimensions was transferred to model_kwargs.\n",
      "                    Please confirm that dimensions is what you intended.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting embeddings\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "data = load_document('LLM Engineers Handbook.pdf')\n",
    "\n",
    "chunks = chunk_data(data)\n",
    "\n",
    "vector_store = insert_embeddings_chromadb(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7de8dec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: how many pages are there in this document?\n",
      "Assistant: I don't know.\n",
      "Human: how many samples Meta reported for Llama 3 through the entire fine-tuning process?\n",
      "Assistant: Meta reported using 10 million samples for Llama 3 throughout the entire fine-tuning process.\n",
      "Human: Multiply this number by 2?\n",
      "Assistant: I don't have access to specific numbers regarding the number of samples Meta reported for Llama 3 throughout the entire fine-tuning process. Therefore, I cannot provide the result of multiplying that number by 2.\n",
      "Human: Multiply the sample size by number by 2?\n",
      "Assistant: I don't have access to specific reports or data from Meta regarding the sample size for Llama 3 or any other specific details about their research or models that aren't publicly disclosed or widely known as of my last update. Therefore, I can't provide the result of multiplying the sample size Meta reported for Llama 3 by 2 without knowing the original sample size.\n",
      "Human: Multiply the result by 2?\n",
      "Assistant: B\n",
      "Follow Up Input: how much is approximate AWS cost for sagemaker service?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "All the cloud services used across the book stick to their freemium option, except AWS. \n",
      "Thus, if you use a personal AWS account, you will be responsible for AWS costs as you \n",
      "follow along in this book. While some services may fall under AWS Free Tier usage, \n",
      "others will not. Thus, you are responsible for checking your billing console regularly.\n",
      "Most of the costs will come when testing SageMaker for training and inference. Based \n",
      "on our tests, the AWS costs can vary between $50 and $100 using the specifications \n",
      "provided in this book and repository.\n",
      "See the AWS documentation on setting up billing alarms to monitor your costs \n",
      "at https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/\n",
      "monitor_estimated_charges_with_cloudwatch.html.\n",
      "\n",
      "All the cloud services used across the book stick to their freemium option, except AWS. \n",
      "Thus, if you use a personal AWS account, you will be responsible for AWS costs as you \n",
      "follow along in this book. While some services may fall under AWS Free Tier usage, \n",
      "others will not. Thus, you are responsible for checking your billing console regularly.\n",
      "Most of the costs will come when testing SageMaker for training and inference. Based \n",
      "on our tests, the AWS costs can vary between $50 and $100 using the specifications \n",
      "provided in this book and repository.\n",
      "See the AWS documentation on setting up billing alarms to monitor your costs \n",
      "at https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/\n",
      "monitor_estimated_charges_with_cloudwatch.html.\n",
      "\n",
      "comfortable working with cloud platforms such as AWS. SageMaker is a double-edged sword \n",
      "regarding costs, following a pay-as-you-go pricing model similar to most AWS services. This \n",
      "means you have to pay for the usage of computing resources, storage, and any other services \n",
      "required to build your applications.\n",
      "In contrast to Bedrock, even if the SageMaker endpoint is not used, you will still pay for the \n",
      "deployed resources on AWS, such as online EC2 instances. Thus, you have to design autoscaling \n",
      "systems that delete unused resources. To conclude, Bedrock offers an out-of-the-box solution \n",
      "that allows you to quickly deploy an API endpoint powered by one of the available foundation\n",
      "Human: What is the approximate AWS cost for using SageMaker service?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The approximate AWS cost for using SageMaker, based on the specifications provided in the book and repository, can vary between $50 and $100. This estimate includes costs associated with training and inference using SageMaker. Keep in mind that actual costs may vary based on your specific usage patterns, the resources you deploy, and the duration for which those resources are utilized.\n"
     ]
    }
   ],
   "source": [
    "question = 'how much is approximate AWS cost for sagemaker service?'\n",
    "\n",
    "result = ask_question_conversation(question,chain)\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a68bf1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mGiven the following conversation and a follow up question, rephrase the follow up question to be a standalone question, in its original language.\n",
      "\n",
      "Chat History:\n",
      "\n",
      "Human: how many pages are there in this document?\n",
      "Assistant: I don't know.\n",
      "Human: how many samples Meta reported for Llama 3 through the entire fine-tuning process?\n",
      "Assistant: Meta reported using 10 million samples for Llama 3 throughout the entire fine-tuning process.\n",
      "Human: Multiply this number by 2?\n",
      "Assistant: I don't have access to specific numbers regarding the number of samples Meta reported for Llama 3 throughout the entire fine-tuning process. Therefore, I cannot provide the result of multiplying that number by 2.\n",
      "Human: Multiply the sample size by number by 2?\n",
      "Assistant: I don't have access to specific reports or data from Meta regarding the sample size for Llama 3 or any other specific details about their research or models that aren't publicly disclosed or widely known as of my last update. Therefore, I can't provide the result of multiplying the sample size Meta reported for Llama 3 by 2 without knowing the original sample size.\n",
      "Human: Multiply the result by 2?\n",
      "Assistant: B\n",
      "Human: how much is approximate AWS cost for sagemaker service?\n",
      "Assistant: The approximate AWS cost for using SageMaker, based on the specifications provided in the book and repository, can vary between $50 and $100. This estimate includes costs associated with training and inference using SageMaker. Keep in mind that actual costs may vary based on your specific usage patterns, the resources you deploy, and the duration for which those resources are utilized.\n",
      "Follow Up Input: if we assume higher cost for sagemaker service, and apply it for two accounts , how much will be grand total?\n",
      "Standalone question:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: Use the following pieces of context to answer the user's question.\n",
      "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "----------------\n",
      "All the cloud services used across the book stick to their freemium option, except AWS. \n",
      "Thus, if you use a personal AWS account, you will be responsible for AWS costs as you \n",
      "follow along in this book. While some services may fall under AWS Free Tier usage, \n",
      "others will not. Thus, you are responsible for checking your billing console regularly.\n",
      "Most of the costs will come when testing SageMaker for training and inference. Based \n",
      "on our tests, the AWS costs can vary between $50 and $100 using the specifications \n",
      "provided in this book and repository.\n",
      "See the AWS documentation on setting up billing alarms to monitor your costs \n",
      "at https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/\n",
      "monitor_estimated_charges_with_cloudwatch.html.\n",
      "\n",
      "All the cloud services used across the book stick to their freemium option, except AWS. \n",
      "Thus, if you use a personal AWS account, you will be responsible for AWS costs as you \n",
      "follow along in this book. While some services may fall under AWS Free Tier usage, \n",
      "others will not. Thus, you are responsible for checking your billing console regularly.\n",
      "Most of the costs will come when testing SageMaker for training and inference. Based \n",
      "on our tests, the AWS costs can vary between $50 and $100 using the specifications \n",
      "provided in this book and repository.\n",
      "See the AWS documentation on setting up billing alarms to monitor your costs \n",
      "at https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/\n",
      "monitor_estimated_charges_with_cloudwatch.html.\n",
      "\n",
      "3. Deploy the ZenML pipelines, container, and artifact registry to AWS.\n",
      "4. Containerize the code and push the Docker image to a container registry.\n",
      "Note that the training and inference pipelines already work with AWS SageMaker. Thus, by fol -\n",
      "lowing the preceding four steps, we ensure that our whole system is on the cloud, ready to scale \n",
      "and serve our imaginary clients.\n",
      "What are the deployment costs?\n",
      "We will stick to the free versions of the MongoDB, Qdrant, and ZenML services. As \n",
      "for AWS, we will mostly stick to their free tier for running the ZenML pipelines. The \n",
      "SageMaker training and inference components are more costly to run (which we \n",
      "wont run in this section). Thus, what we will show you in the following sections\n",
      "Human: What would be the total cost of using AWS SageMaker service for two accounts if we assume the higher cost estimate?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "If we assume the higher cost estimate for using AWS SageMaker as mentioned, which is $100 for the specifications provided in the book and repository, the total cost for two accounts would be:\n",
      "\n",
      "- For one account: $100\n",
      "- For two accounts: $100 x 2 = $200\n",
      "\n",
      "Therefore, the total cost of using AWS SageMaker service for two accounts, assuming the higher cost estimate, would be $200.\n"
     ]
    }
   ],
   "source": [
    "question = 'if we assume higher cost for sagemaker service, and apply it for two accounts , how much will be grand total?'\n",
    "\n",
    "result = ask_question_conversation(question,chain)\n",
    "\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fb4f6b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "how many pages are there in this document?\n",
      "--------------------------------------------------\n",
      "I don't know.\n",
      "--------------------------------------------------\n",
      "how many samples Meta reported for Llama 3 through the entire fine-tuning process?\n",
      "--------------------------------------------------\n",
      "Meta reported using 10 million samples for Llama 3 throughout the entire fine-tuning process.\n",
      "--------------------------------------------------\n",
      "Multiply this number by 2?\n",
      "--------------------------------------------------\n",
      "I don't have access to specific numbers regarding the number of samples Meta reported for Llama 3 throughout the entire fine-tuning process. Therefore, I cannot provide the result of multiplying that number by 2.\n",
      "--------------------------------------------------\n",
      "Multiply the sample size by number by 2?\n",
      "--------------------------------------------------\n",
      "I don't have access to specific reports or data from Meta regarding the sample size for Llama 3 or any other specific details about their research or models that aren't publicly disclosed or widely known as of my last update. Therefore, I can't provide the result of multiplying the sample size Meta reported for Llama 3 by 2 without knowing the original sample size.\n",
      "--------------------------------------------------\n",
      "Multiply the result by 2?\n",
      "--------------------------------------------------\n",
      "B\n",
      "--------------------------------------------------\n",
      "how much is approximate AWS cost for sagemaker service?\n",
      "--------------------------------------------------\n",
      "The approximate AWS cost for using SageMaker, based on the specifications provided in the book and repository, can vary between $50 and $100. This estimate includes costs associated with training and inference using SageMaker. Keep in mind that actual costs may vary based on your specific usage patterns, the resources you deploy, and the duration for which those resources are utilized.\n",
      "--------------------------------------------------\n",
      "if we assume higher cost for sagemaker service, and apply it for two accounts , how much will be grand total?\n",
      "--------------------------------------------------\n",
      "If we assume the higher cost estimate for using AWS SageMaker as mentioned, which is $100 for the specifications provided in the book and repository, the total cost for two accounts would be:\n",
      "\n",
      "- For one account: $100\n",
      "- For two accounts: $100 x 2 = $200\n",
      "\n",
      "Therefore, the total cost of using AWS SageMaker service for two accounts, assuming the higher cost estimate, would be $200.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for item in result['chat_history']:\n",
    "    print(item.content)\n",
    "    print('-'*50)\n",
    "    print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce91d42a",
   "metadata": {},
   "source": [
    "# Using custom prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a558dd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_classic.chains import ConversationalRetrievalChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_classic.memory import ConversationBufferMemory\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate,HumanMessagePromptTemplate,SystemMessagePromptTemplate\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model='gpt-4-turbo-preview',temperature=0)\n",
    "\n",
    "retriever = vector_store.as_retriever(search_type='similarity',search_kwargs={'k':3})\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "\n",
    "system_template = r'''\n",
    "use following pieces of context to answer the question at the end.If you dont find answer in context, say \"This is outside of pdf provided context\" \n",
    "and then answer the question\n",
    "\n",
    "context : ```{context}```\n",
    "\n",
    "Question: ```{question}```\n",
    "Answer: Let's think step by step,\n",
    "\n",
    "'''\n",
    "\n",
    "user_template = '''\n",
    "Question:```{question}```\n",
    "chat history:```{chat_history}```\n",
    "'''\n",
    "\n",
    "messages = [\n",
    "    SystemMessagePromptTemplate.from_template(system_template),\n",
    "    HumanMessagePromptTemplate.from_template(user_template)\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "\n",
    "\n",
    "\n",
    "chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm, \n",
    "    chain_type='stuff',\n",
    "    memory=memory,\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain_kwargs={'prompt':prompt},\n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ee4be920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['chat_history', 'context', 'question'] input_types={} partial_variables={} messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"\\nuse following pieces of context to answer the question at the end.\\n\\ncontext : ```{context}```\\n\\nQuestion: ```{question}```\\nAnswer: Let's think step by step,\\n\\n\"), additional_kwargs={}), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['chat_history', 'question'], input_types={}, partial_variables={}, template='\\nQuestion:```{question}```\\nchat history:```{chat_history}```\\n'), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b3d639fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: \n",
      "use following pieces of context to answer the question at the end.If you dont find answer in context, say \"This is outside of pdf provided context\" \n",
      "and then answer the question\n",
      "\n",
      "context : ```approximate nearest neighbor (ANN) algorithms to find these close neighbors.\n",
      "While ANN algorithms dont return the top matches for a given search, standard nearest neigh -\n",
      "bor algorithms are too slow to work in practice. Also, it is shown empirically that using only ap-\n",
      "proximations of the top matches for a given input query works well enough. Thus, the trade-off \n",
      "between accuracy and latency ultimately favors ANN algorithms.\n",
      "This is a typical workflow of a vector DB:\n",
      "1. Indexing vectors: Vectors are indexed using data structures optimized for high-dimen -\n",
      "sional data. Common indexing techniques include hierarchical navigable small world\n",
      "(HNSW), random projection, product quantization (PQ), and locality-sensitive hashing\n",
      "(LSH).\n",
      "\n",
      "approximate nearest neighbor (ANN) algorithms to find these close neighbors.\n",
      "While ANN algorithms dont return the top matches for a given search, standard nearest neigh -\n",
      "bor algorithms are too slow to work in practice. Also, it is shown empirically that using only ap-\n",
      "proximations of the top matches for a given input query works well enough. Thus, the trade-off \n",
      "between accuracy and latency ultimately favors ANN algorithms.\n",
      "This is a typical workflow of a vector DB:\n",
      "1. Indexing vectors: Vectors are indexed using data structures optimized for high-dimen -\n",
      "sional data. Common indexing techniques include hierarchical navigable small world\n",
      "(HNSW), random projection, product quantization (PQ), and locality-sensitive hashing\n",
      "(LSH).\n",
      "\n",
      "OpenAIs models quickly evolve, the model might get deprecated. But thats not an issue, as you \n",
      "can quickly change it in your .env file by configuring the OPENAI_MODEL_ID environment variable.\n",
      "Query expansion\n",
      "The problem in a typical retrieval step is that you query your vector DB using a single vector rep-\n",
      "resentation of your original question. This approach covers only a small area of the embedding \n",
      "space, which can be limiting. If the embedding doesnt contain all the required information or \n",
      "nuances of your query, the retrieved context may not be relevant. This means essential documents \n",
      "that are semantically related but not near the query vector might be overlooked.```\n",
      "\n",
      "Question: ```What is vertex AI search?```\n",
      "Answer: Let's think step by step,\n",
      "\n",
      "\n",
      "Human: \n",
      "Question:```What is vertex AI search?```\n",
      "chat history:``````\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "This is outside of the provided context.\n",
      "\n",
      "Vertex AI Search is not mentioned in the provided context. However, based on my knowledge, Vertex AI is a managed machine learning platform by Google that helps in building, deploying, and scaling ML models. It offers various services and tools for AI development, including AutoML, AI Platform (Unified), and custom model development tools. If Vertex AI Search refers to a specific feature or service, it would likely be related to searching and managing datasets, models, or other AI-related resources within the Vertex AI ecosystem. For detailed and accurate information, referring to the official Google Cloud documentation or announcements would be necessary.\n"
     ]
    }
   ],
   "source": [
    "db = get_embeddings_chromadb()\n",
    "\n",
    "\n",
    "question = 'What is vertex AI search?'\n",
    "\n",
    "result = ask_question_conversation(question,chain)\n",
    "\n",
    "print(result['answer'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
